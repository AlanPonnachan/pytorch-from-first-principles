{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a16584",
   "metadata": {},
   "source": [
    "Often We need to organize our code into modules that PyTorch can track, save, and move to GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7d5257",
   "metadata": {},
   "source": [
    "## Problem 1: The \"MyLinear\" Layer from Scratch\n",
    "\n",
    "In Notebook 03, we used `w` and `b` as loose tensors. Now, we will build a proper Layer. You are **not** allowed to use `nn.Linear`. You must use `nn.Parameter`.\n",
    "\n",
    "\n",
    "\n",
    "**Your Challenge:**\n",
    "Implement a class `MyLinear` that mimics `nn.Linear`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1987ba74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([5, 3])\n",
      "model state keys: ['weights', 'bias']\n",
      "no of model params: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLinear(nn.Module): \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__() \n",
    "        # Initialize smaller weights for stability\n",
    "        self.weights = nn.Parameter(torch.randn(in_dim, out_dim) * 0.01)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # b = batch, i = in_dim, j = out_dim\n",
    "        y_pred = torch.einsum('bi,ij->bj', x, self.weights) + self.bias\n",
    "        return y_pred\n",
    "\n",
    "# Test it with a Batch of 5:\n",
    "model = MyLinear(10, 3)\n",
    "x = torch.randn(5, 10) # 5 samples, 10 features each\n",
    "y_pred = model(x)\n",
    "print(f\"Output shape: {y_pred.shape}\") # Should be (5, 3)\n",
    "\n",
    "print(f\"model state keys: {list(model.state_dict().keys())}\")\n",
    "print(f\"no of model params: {len(list(model.parameters()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f780ad",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Post-Mortem (Problem 1)</summary>\n",
    "\n",
    "#### 1. Why is the count only 2?\n",
    "In PyTorch, `model.parameters()` returns a list of **Tensors** (specifically `nn.Parameter` objects).\n",
    "1.  `self.weights` is **one** tensor object (of shape $10 \\times 3$).\n",
    "2.  `self.bias` is **one** tensor object (of shape $3$).\n",
    "\n",
    "Even though there are 33 individual floating-point numbers (elements) inside, PyTorch counts them as 2 \"tensors to be optimized.\" \n",
    "\n",
    "**To see the total number of individual numbers (scalar elements), you have to do this:**\n",
    "```python\n",
    "total_elements = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total scalar elements: {total_elements}\") # This will be 33\n",
    "```\n",
    "\n",
    "#### 2. The \"Ghost\" Experiment Analysis\n",
    "If you added `self.noise = torch.randn(10)`:\n",
    "*   It is **NOT** in the `state_dict`.\n",
    "*   It is **NOT** in `parameters()`.\n",
    "*   **Why?** Because it is just a standard Python attribute. PyTorch's `nn.Module` has a \"magic\" `__setattr__` method. When you write `self.x = ...`, PyTorch checks: *\"Is this an `nn.Module` or an `nn.Parameter`?\"* If yes, it adds it to the internal tracker. If not, it ignores it.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900e85f",
   "metadata": {},
   "source": [
    "### Problem 2: The Multi-Layer Perceptron (Stacking)\n",
    "\n",
    "\n",
    "\n",
    "**Your Challenge:**\n",
    "Use your `MyLinear` class to build a 2-layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa3b518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP state keys: ['fc1.weights', 'fc1.bias', 'fc2.weights', 'fc2.bias']\n",
      "Total MLP parameter objects: 4\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # 1. Define two layers using your MyLinear class\n",
    "        self.fc1 = MyLinear(in_dim, hidden_dim)\n",
    "        self.fc2 = MyLinear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 2. Implement: Input -> Layer 1 -> ReLU -> Layer 2\n",
    "        # Use torch.relu() or torch.nn.functional.relu()\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 3. Create the MLP: 10 inputs -> 20 hidden -> 1 output\n",
    "mlp = SimpleMLP(10, 20, 1)\n",
    "\n",
    "# TEST: Run the parameter check again!\n",
    "print(f\"MLP state keys: {list(mlp.state_dict().keys())}\")\n",
    "print(f\"Total MLP parameter objects: {len(list(mlp.parameters()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42967f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
