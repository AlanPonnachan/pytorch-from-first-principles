{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ea3e95",
   "metadata": {},
   "source": [
    "# Memory & Strides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8639f",
   "metadata": {},
   "source": [
    "## 1. Problem Statement: The Transpose \n",
    "You are given a 2D tensor $M$ of shape (3, 2). \n",
    "1. Manually define its \"Storage\" (the flat array in memory).\n",
    "2. Change its shape without moving the numbers in memory.\n",
    "3. Transpose it and explain why PyTorch says the data hasn't moved, even though the shape changed.\n",
    "\n",
    "### The Setup\n",
    "```python\n",
    "# !pip install -q matplotlib\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create a 3x2 tensor\n",
    "# [ 1, 2 ]\n",
    "# [ 3, 4 ]\n",
    "# [ 5, 6 ]\n",
    "M = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "```\n",
    "\n",
    "### Your Challenge:\n",
    "1. **The Metadata:** Access `M.storage()`, `M.stride()`, and `M.storage_offset()`.\n",
    "2. **The Prediction:** If I call `M_t = M.t()` (transpose), what will be the new `stride`? \n",
    "3. **The Proof:** Modify a single value in `M`. Does it change in `M_t`? Why?\n",
    "4. **The \"Contiguous\" Trap:** Try to call `M_t.view(-1)`. It will fail. **Why?** And how do you fix it from a first-principle perspective?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb5acfa",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "\n",
    "*   **Storage:** The actual 1D array in your RAM: `[1, 2, 3, 4, 5, 6]`.\n",
    "*   **Stride:**  It tells PyTorch: \"To move to the next row, skip $X$ elements. To move to the next column, skip $Y$ elements.\"\n",
    "*   **View vs. Reshape:** One is a \"Metadata change,\" the other might \"Copy data.\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a725329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as numpy\n",
    "\n",
    "M = torch.tensor ([[1,2], [3,4], [5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d76c98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " M.storage:  1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]  \n",
      " M.stride: (2, 1) \n",
      " M.storage_offset: 0 \n",
      " M.size(): torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "print(f\" M.storage: {M.storage()}  \\n M.stride: {M.stride()} \\n M.storage_offset: {M.storage_offset()} \\n M.size(): {M.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e249956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_t: tensor([[1, 3, 5],\n",
      "        [2, 4, 6]]) \n",
      " M_t.storage:  1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]  \n",
      " M_t.stride: (1, 2) \n",
      " M_t.storage_offset: 0 M_t.size(): torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "M_t = M.t()\n",
    "print(f\"M_t: {M_t} \\n M_t.storage: {M_t.storage()}  \\n M_t.stride: {M_t.stride()} \\n M_t.storage_offset: {M_t.storage_offset()} M_t.size(): {M_t.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e71d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After changing M[0,1] to 20: \n",
      " M: tensor([[ 1, 20],\n",
      "        [ 3,  4],\n",
      "        [ 5,  6]]) \n",
      " M_t: tensor([[ 1,  3,  5],\n",
      "        [20,  4,  6]])\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "# changing [0,1] element of M\n",
    "M[0,1] = 20\n",
    "print(f\"After changing M[0,1] to 20: \\n M: {M} \\n M_t: {M_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a44afc88",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#4\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mM_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "#4\n",
    "print(M_t.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4811bf36",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "#### The Stride Math \n",
    "output showed:\n",
    "*   `M.stride()`: `(2, 1)` $\\rightarrow$ To move 1 row, jump 2 elements. To move 1 col, jump 1.\n",
    "*   `M_t.stride()`: `(1, 2)` $\\rightarrow$ To move 1 row, jump **1** element. To move 1 col, jump **2**.\n",
    "\n",
    " Transposing in PyTorch is **Zero-Latency**. It doesn't move a single byte in RAM. It just swaps the stride numbers. \n",
    "\n",
    "#### Why did `.view()` fail?\n",
    "The `.view()` method has a strict contract: **The logical order must match the physical order.**\n",
    "*   In `M`, the physical storage is `[1, 2, 3, 4, 5, 6]`. To read it logically, you go `1, 2` (row 1), then `3, 4` (row 2). This matches the storage perfectly. It is **Contiguous**.\n",
    "*   In `M_t`, the logical order is `[1, 3, 5]` for the first row. But in physical memory, `1` and `3` are not neighbors (`2` is between them). This is **Non-Contiguous**.\n",
    "\n",
    "**The Fix:** `M_t.contiguous().view(-1)`\n",
    "`.contiguous()` is the command that says: *\"Okay PyTorch, actually move the numbers in RAM so they match my current logical view.\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e2a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
